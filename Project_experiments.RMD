---
title: "Practical Machine Learning Project"
author: "Rafael Espericueta"
date: "08/17/2014"
output: html_document
---

### Overview
Human Activity Recognition, **HAR**, is a currently an active area of reasearch. In this project we explored various machine learning algorithms to classify from sets of measurements in the HAR Dataset for Benchmarking if a curl with dumbells was done correctly, or if not, in what way. Our data is from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available here: 
<http://groupware.les.inf.puc-rio.br/har> (see the section on the Weight Lifting Exercise Dataset).

### Data Munging

The following is the code used to download and read the data files into RStudio:

```{r results='hide', message=FALSE, warning=FALSE}

library(RCurl)

# The training data for this project are available here: 
trainURL <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv" 
# The test data are available here: 
testURL <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"

# If needed, create a new data directory.
if (!file.exists("data"))  { dir.create("data")}

# Download and read the training and test data.
train_filename <- "./data/train.csv"; test_filename <- "./data/test.csv"
#download.file(trainURL, destfile = train_filename, method="curl")
#download.file(testURL, destfile = test_filename, method="curl")
#dateDownloaded <- date()
alldat <- read.csv(train_filename);  final_testdat <- read.csv(test_filename)
# Take a look at the data...
str(alldat)
```

There seem to be many columns that consist of nothing but NA's, and some that consist mostly of "". These variables will be eliminated as potential predictors since they hold little information, and including them will tend to needlessly increase the variance of our model.

```{r}
unique(colSums(is.na(alldat)))
```

Indeed, each variable has either 0 or 19216 NA's, which means each column has zero NAs or is 98% NA's. 

```{r}
unique(colSums(alldat[sapply(alldat, is.factor)] == ""))
```

Similarly, we also find factor columns where 98% of the columns are "". We now delete these information impoverished columns.

```{r}
i <- colSums(is.na(alldat)) > 100
alldat <- alldat[, !i];  final_testdat <- final_testdat[, !i]

i <- colSums(alldat == "") > 100
alldat <- alldat[, !i];  final_testdat <- final_testdat[, !i]
```

There are also 3 columns of data relating to time; we'll delete them all, as time shouldn't be a relevant variable for this data. Also the first column, x, is just the row number, so we'll delete that too, along with the subject's name. There are 
also two variables dealing with "windows" that don't seem particularly relevant to the task at hand, so they go too.

```{r}
cols_to_delete <- 1:7
alldat <- alldat[, -cols_to_delete];  final_testdat <- final_testdat[, -cols_to_delete]
final_testdat$problem_id <- NULL   # These are just the row numbers again!
```

Variables that are highly correlated are problematic when it comes to creating predictive models, so we will also delete such redundancies. Whenever two columns are correlated at 0.9 or higher, one will be deleted.

```{r}
library(caret)
dat <- alldat[, -53]   # the last column (classe) is a factor
descrCorr <- cor(dat)
highCorr <- findCorrelation(descrCorr, 0.9)  # this function returns the indices of the columns to be culled
dat <- dat[, -highCorr]
# We need the first and last columns..
alldat <- cbind(dat, classe = alldat$classe)
# We must also do this to the final test set.
final_testdat <- final_testdat[, -highCorr]
```

After all that carnage, we are left with 45 covariates along with our predictor variable, classe. Next we'll split our data set into a training and a test set (not to be confused with our final test set of 20 observations).

```{r results='hide', message=FALSE, warning=FALSE}
set.seed(2718)
trainIndex = createDataPartition(alldat$classe, p = 0.70, list = FALSE)
traindat <- alldat[trainIndex,]; testdat <- alldat[-trainIndex,]
```


### Model Construction

Since the predictor variable is categorical, we'll use a random forest model. With these models there's no preprocessing needed as there is for many other models (like neural networks, SVM, etc.) where the data must be centered first.

To speed things up, we first load a parallel processing library. Strangely, this worked well on my Ubuntu 14.04 system with 16 cores, but didn't use the extras cores when I ran it on my Ubuntu 12.10 laptop with 8 cores.
```{r}
#install.packages("doMC")
library(doMC)
registerDoMC(cores = 10)
```

Our random forest model use the caret function "train"'s default settings, as is recommended. Caret's train function only allows for the parameter "mtry" (the number of features to fit per tree) to be fit, and since the default is the floor function of the square root of the number of columns (in this case 7), we used cross validation with its default settings (10-fold cross validation) to see if we could find a better value for "mtry" near the default value of 7.

```{r}
set.seed(2014)

my.grid <- expand.grid(.mtry = c(5, 6, 7, 8, 9))

rfTrain <- train(classe ~ .,
                 data = traindat,
                 method = "rf",
                 trControl = trainControl(method = "cv"),
                 tuneGrid = my.grid
                 )
rfTrain
```

Cross validation reveals that the default value for the number of features to fit per tree, which for this dataset is 7, was indeed the optimal one. We also see high accuracies of over 99%! Let's see how well our random forest model predicts the training set.

```{r}
traindat.predictions <- predict(rfTrain)
sum(traindat$classe != traindat.predictions)  # 0, perfect predictions!
```

Our model has 0 errors on the training set. Next we'll see how our model does predicting our set-aside labeled test set.
 
```{r}
testdat.predictions <- predict(rfTrain, newdata = testdat[, -46])
testaccuracy <- 1 - sum(testdat$classe != testdat.predictions) / length(testdat$classe)
testaccuracy
confusionMatrix(data=testdat$classe, testdat.predictions)
```

Our random forest model attained 99.4% accuracy on our test set. The confusion matrix shows that the most common classification errors involve labeling C's as D's, but the overall sensitivity of 99.8% and specificity of 99.9% look great! Finally we output our model's prediction on the final test set of 20 cases for submission.

```{r}
final_testdat.predictions  <- predict(rfTrain, newdata = final_testdat)
final_testdat.predictions
```

Random forests on the training data yielded an accuracy of 99.2%.  
Final test set predictions:  
B A B A A E D B A A B C B A E E A B B B           

Finally we use the entire dataset for training (except for the final test set for submission).

```{r}
set.seed(2014)
rfTrain <- train(classe ~ .,
                 data = alldat,
                 method = "rf"
                 )
rfAll <- rfTrain
rfAll
alldat.predictions <- predict(rfAll)
sum(alldat$classe != alldat.predictions)  # 0, perfect predictions!

final_testdat.predictions  <- predict(rfAll, newdata = final_testdat)
final_testdat.predictions
```

This agrees with our previous predictions!


Finally we use the entire dataset for training (except for the final test set for submission).

```{r}
set.seed(2014)
my.grid <- expand.grid(.mtry = c(5, 6, 7, 8, 9))

rfTrain <- train(classe ~ .,
                 data = alldat,
                 method = "rf",
                 trControl = trainControl(method = "cv"),
                 tuneGrid = my.grid
                 )
rfAll <- rfTrain
rfAll

alldat.predictions <- predict(rfAll)
sum(alldat$classe != alldat.predictions)  # 0, perfect predictions!

final_testdat.predictions  <- predict(rfAll, newdata = final_testdat)
final_testdat.predictions
```

This agrees with our previous predictions!




Neural Nets (nnet)  decay, size

```{r}
set.seed(2014)

my.grid <- expand.grid(.decay = c(0., 0.1, 0.5), .size = c(10, 20, 30))
nnTrain <- train(classe ~ .,
                 data = traindat,
                 method = "nnet",
                 preProcess = c("center", "scale"),
                 trControl = trainControl(method = "cv"),
                 maxit = 1000,
                 tuneGrid = my.grid,
                 trace = F,
                 )
nnTrain
traindat.predictions <- predict(rfTrain)
sum(traindat$classe != traindat.predictions)  # 0, perfect predictions!
 
testdat.predictions <- predict(rfTrain, newdata = testdat[,-46])
testaccuracy <- 1 - sum(testdat$classe != testdat.predictions) / length(testdat$classe)
testaccuracy
confusionMatrix(data=testdat$classe, testdat.predictions)

final_testdat.predictions  <- predict(rfTrain, newdata = final_testdat)
final_testdat.predictions
```

answers <- as.character(final_testdat.predictions)

# The following code was provided in the assignment instructions 
# to generate the required output files.
pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}

pml_write_files(answers)






Generalized Boosted Regression Models

```{r}
gbmGrid <- expand.grid(.n.trees = c(125, 150, 175),
                       .interaction.depth = 1:3,
                       .shrinkage = c(0.3, .5, .6))

gbmFitTrain <- train(classe ~ .,
                     data = traindat[, -c(1, 2)],  # throw out windows columns
                     method = "gbm",
                     distribution = "multinomial",
                     tuneGrid = gbmGrid,
                     trControl = trainControl(method = "cv")  # "all" returns probs?
                   )
gbmFitTrain
ls(gbmFitTrain)
gbmFitTrain$results
gbmFitTrain$bestTune
gbmFitTrain$modelInfo
gbmFitTrain$finalModel
gbmFitTrain$pred


gbmFitTrain$finalModel$fit
ls(gbmFitTrain$finalModel)
gbmFitTrain$finalModel$estimator
#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
predtrain <- predict(gbmFitTrain, type="raw")  # This worked,  100% predicted correctly in training set.
sum(pred != y)

predtest <- predict(gbmFitTrain, newdata = testdat[, -55], type="raw")

# 
summary(predict(modFitAll$finalModel, type="response", n.trees=100))  # data = traindat[,-57], 



x <- data.frame('A' = 19, 'B' = 3, 'C' = 1, 'D' = 4)
y <- data.frame('A' = 4, 'B' = 1, 'C' = 10, 'D' = 5)
x <- rbind(x, y)
x$class <- ""
numrows <- dim(x)[1]
for (i in 1:numrows) {
  x[i, dim(x)[2]] <- names(x[i,])[which.max(x[i,])]
}

pred <- as.data.frame(pred)
names(pred) <- c("A", "B", "C", "D", "E")
pred$class <- ""
numrows <- dim(pred)[1]
for (i in 1:numrows) {
  pred[i, dim(pred)[2]] <- as.character(names(pred[i,])[which.max(pred[i,])])
}
pred$class <- factor(pred$class)
summary(pred)




```



This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r}
data(cars)
summary(cars)
```

You can also embed plots, for example:

```{r, echo=FALSE}
plot(cars)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
